{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 학습 목표\n",
    "\n",
    "+ 분류를 위해 확률 분포 사용하기\n",
    "\n",
    "+ 나이브 베이스 분류기 학습하기\n",
    "\n",
    "+ RSS 피드에서 제공되는 데이터 구문 분석하기\n",
    "\n",
    "+ 지역적인 태도를 알아보기 위해 나이브 베이스 사용하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 많은 머신러닝 알고리즘은 확률 이론에 기반을 두고 있다. 주어진 값이 나타나는 속성에 대한 확률을 계산할 때, 3장에서 다루었던 부분을 확률로 다루어볼 것이다. \n",
    "\n",
    "2. 간단한 확률적 분류기를 가지고 시작하여, 약간의 가정을 세우고 나이브 베이즈 분류기를 학습한다. \"나이브\"라고 하는 것은 이 공식이 몇 가지 나이브한 가정을 이야기 함\n",
    "\n",
    "3. 문서를 단어 벡터로 분할하기 위해 파이썬이 가진 텍스트 처리의 장점을 가능한 최대로 활용할 것이다.\n",
    "즉, 파이썬을 텍스트 분류에 사용할 것이다. 또 다른 분류기를 구축하고 이것이 실제 스펨 이메일 데이터 집합에서 어떻게 사용되는지 확인할 것 이다.\n",
    "\n",
    "4. 조건부확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 베이지안 의사결정 이론으로 분류하기\n",
    "\n",
    "> ** _ 나이브 베이즈 _**\n",
    "\n",
    ">**장점** : 소량의 데이터를 가지고 작업이 이루어지며, 여러 개의 분류 항목을 다룰 수 있다.\n",
    ">\n",
    ">**단점** : 입력 데이터를 어떻게 준비하느냐에 따라 민감하게 작용한다.\n",
    ">\n",
    ">**적용** : 명목형 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 분류 항목에 통계적 매개변수를 찾았다고 해 보자. 우리는 분류 항목 1에 속하는 데이터의 확률 방정식 (원): $p1(x,y)$ 와 분류 항목 2에 속하는 데이터 확률 방정식(세모) $p2(x,y)$가 있다. 속성 (x, y)를 가지고 새로운 측정으로 분류하기 위해서 다음과 같은 규칙을 사용한다. \n",
    "\n",
    "만약에 $p1(x,y)$ > $p2(x,y)$이면, 분류 항목 1에 속한다.\n",
    "\n",
    "만약에 $p2(x,y)$ > $p1(x,y)$이면, 분류 항목 1에 속한다.\n",
    "\n",
    "간단하게 말하면, **더 높은 확률을 가지는 분류 항목을 선택한다.** 베이즈 정리 이론은 더 높은 확률을 가지는 의사결정을 선택하는 것이다.\n",
    "그림 4.1에 있는 데이터로 돌아가보자. 만약에 우리가 소수점 여섯 자리의 수로 데이터를 표현할 수 있고, 확률을 계산하는 코드가 파이썬에서 2줄 표현이 가능하다면, 우리에게 이보다 더 좋은 일이 있을까?\n",
    "\n",
    "1. 1장에 있는 kNN을 사용하여 1,000개의 거리 계산을 수행\n",
    "\n",
    "2. 2장에 있는 의사결정 트리를 사용하여, x축을 따르는 데이터인지 y축을 따르는 데이터인지 분할한다.\n",
    "\n",
    "3. 각 분류 항목의 확률을 계산하고 이들을 비교한다.\n",
    "\n",
    "의사결정 트리는 매우 성공적이지 못하며, kNN은 간단한 확률 계산을 비교하는 많은 계산이 요구된다. 이러한 문제를 감안할 떄 가장 좋은 선택은 곧 다루게 될 확률적인 비교이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 조건부 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "p(c_i|x,y) = \\frac{p(x,y|c_i)p(c_i)}{p(x,y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 조건부 확률 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "p(c_1|x,y) > p(c_2|x,y), 분류 항목 c_1에 속함\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(c_2|x,y) > p(c_1|x,y), 분류 항목 c_2에 속함\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 나이브 베이즈로 문서 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** _ 나이브 베이즈에 대한 일반적인 접근 방법 _**\n",
    "\n",
    ">**수집** : 많은 방법이 있다. 이번 장에서는 RSS 자료를 사용할 것이다.\n",
    ">\n",
    ">**준비** : 명목형 또는 부울 형(Boolean) 값이 요구된다.\n",
    ">\n",
    ">**분석** : 많은 속성들을 플롯하는 것은 도움이 되지 못한다.히스토그램으로 보는 것이 가장 좋다.\n",
    ">\n",
    ">**훈련** : 각 속성을 독립적으로 조건부 확률을 계산한다.\n",
    ">\n",
    ">**검사** : 오분류율을 계산한다.\n",
    ">\n",
    ">**사용** : 나이브 베이스의 일반적인 응용 프로그램 중 하나는 문서 분류이다. 어떤 분류를 설정하는 데 있어 나이브 베이스를 사용할 수 있다. 그것이 꼭 텍스트일 필요는 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 파이썬으로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 준비 : 텍스로 단어 벡터 만들기\n",
    "\n",
    "#### [리스팅 4.1] 벡터 함수의 단어 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]    #1 is abusive, 0 not\n",
    "    return postingList, classVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([]) # 비어있는 집합 생성\n",
    "    for document in dataSet: \n",
    "        vocabSet = vocabSet | set(document) # loop 돌면서 unique value를 vocabSet에 저장!\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute',\n",
       " 'quit',\n",
       " 'him',\n",
       " 'ate',\n",
       " 'buying',\n",
       " 'dog',\n",
       " 'how',\n",
       " 'licks',\n",
       " 'posting',\n",
       " 'help',\n",
       " 'stupid',\n",
       " 'to',\n",
       " 'flea',\n",
       " 'problems',\n",
       " 'so',\n",
       " 'mr',\n",
       " 'has',\n",
       " 'dalmation',\n",
       " 'love',\n",
       " 'please',\n",
       " 'maybe',\n",
       " 'park',\n",
       " 'I',\n",
       " 'garbage',\n",
       " 'worthless',\n",
       " 'is',\n",
       " 'steak',\n",
       " 'my',\n",
       " 'food',\n",
       " 'stop',\n",
       " 'not',\n",
       " 'take']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 중복된 단어가 없음을 확인!!\n",
    "mVocabList = createVocabList(listOPosts)\n",
    "mVocabList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\ $이 목록을 검토해 보면 중복된 단어가 없다는 것을 알 수 있다. 이 목록은 정렬되지 않았지만, 혹시라도 정렬을 원하면 해도 상관없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(list(mVocabList), listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(mVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\ $ 이 함수는 각 단어의 속성을 확인하고 생성하고자 할 때 어휘 목록 또는 모든 단어의 목록을 구한다. 이제 주어진 문서를 적용하면, 문서는 단위 벡터로 변환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.5.2 훈련 : 단어 벡터로 확률 계산\n",
    "\n",
    "### 4.5.3 검사: 실제 조건을 반영하기 위해 분류기 수정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(c_i|w) =  \\frac{p(w|c_i)p(c_i)}{p(w)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "c_i : 분류 항목(i = 1 폭력) or (i = 0 : 비폭력)\n",
    "$$\n",
    "\n",
    "$$\n",
    "w : 어휘집에 있는 단어들처럼 많은 값\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$ \\ $ 단어가 얼마나 많이 발생하는지를 가지고 $p(c_i)$를 계산할 수 있음. 즉 i번째 분류 항목(폭력적인 글인지 아니면 폭력적이지 않은 글인지)을 확인한 다음 이를 전체 문서의 수로 나눔.\n",
    "\n",
    "**$1.$** 나이브 가정\n",
    "\n",
    "$$\n",
    "p(w|c_i) = p(w_1,w_2,w_3,...,w_N|c_i)\n",
    "$$\n",
    "\n",
    "**$2.$** 모든 단어들이 서로 독립적이라고 가정하면 이를 조건부 독립으로 하여 다음과 같이 펼쳐 쉽게 계산가능\n",
    "\n",
    "\n",
    "$$\n",
    "p(w|c_i) = p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)...p(w_N|c_i)\n",
    "$$\n",
    "\n",
    "**$3.$** Laplace Smoothing/ Log 변환 [문제점 해결]\n",
    "\n",
    "    1. 입력 벡터에 학습 벡터가 제시되지 않은 요소가 존재하면 조건부 확률은 항상 0으로 계산\n",
    "\n",
    "        '첫번째 문제'를 해결 하기 위해 단어의 개수(분자)를 모두 1로 초기화하고, 분모는 2로 초기화한다.\n",
    "\n",
    "$$\n",
    "p(w_k|c_i) = \\frac{1 + count(w_k,c_i)}{2 + \\sum count(w_k,c_i)}\n",
    "$$\n",
    "\n",
    "    2. 입력 벡터를 구성하는 요소가 많으면, 조건부 확률 값이 너무 작아져서 값의 비교가 어려운 underflow 현상 발생\n",
    "\n",
    "        '두번째 문제'를 해결 하기 위해 log변환 이용함\n",
    "\n",
    "$$\n",
    "log(p(w_1|c_i)) + log(p(w_2|c_i)) + log(p(w_3|c_i)) + ... + log(p(w_n|c_i))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [리스팅 4.2] 나이브 베이즈 분류기 train 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "# [1] : 인자 2개!\n",
    "def trainNB0(trainMatrix, trainCategory): \n",
    "    numTrainDocs = len(trainMatrix) # 문서 갯수 : 6\n",
    "    numWords = len(trainMatrix[0]) # 문서 1에 해당하는 list 길이 : 32\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs) # 3/6 = 0.5\n",
    "\n",
    "# [2] 확률 초기화!\n",
    "    # 단어 벡터를 1로 초기화\n",
    "    p0Num = np.ones(numWords) \n",
    "    p1Num = np.ones(numWords)\n",
    "    # 분모를 2로 초기화\n",
    "    p0Denom = 2.0 \n",
    "    p1Denom = 2.0 \n",
    "    \n",
    "# [3] \"분류[1,0]에 따라서 단어가 나올 확률\" 벡터 추가하는 과정    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i]) # 2 + 7 + 8 + 9 \n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "            \n",
    "    p1Vect = np.log(p1Num/p1Denom)          # log(p1Num / 21)\n",
    "    p0Vect = np.log(p0Num/p0Denom)          # log(p1Num / 26)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] : 문서 행렬인  [trainMatrix]와 각 문서에 대한 분류 항목이 저장된 벡터인 [trainCategory]를 인자로 받음\n",
    "\n",
    "#### [2] : 폭력적인 문서(분류 항목의 값이 1인 문서)의 확률을 계산하는 것\n",
    "\n",
    "> $ \\ $ 이 계산을 확률로 표현하면, 분류 항목이 두 개이기 때문에 $p(1)$ 이 된다. 그리고 $p(0)$는 $1-p(1)$으로 구할 수 있다. 분류 항목이 두 개 이상인 경우에는 이 부분을 조금 수정해야 한다.\n",
    "\n",
    "> 1. $p(w_i|c_1)$ 과 $p(w_i|c_0)$을 계산하기 위해서 분자와 분모를 초기화.\n",
    ">\n",
    ">  $ \\ $ 많은 $w$을 가지고 있으므로 빠르게 계산하기 위해 Numpy의 배열을 사용. 분자는 Numpy 배열로 표현되며, 배열의 크기는 마치 어휘집 내에 가지고 있는 단어들처럼 원소의 개수와 같다. 반복문에서는  trainMatrix나 훈련 집합에 있는 모든 문서를 반복한다. 반복을 할 때마다 하나의 단어는 하나의 문서 내에 나타나게 되며, 이때마다 단어의 개수(**p1Num** or **p0Num**)는 증가하게 된다.\n",
    "\n",
    "#### [3] : \"분류[1,0]에 따라서 단어가 나올 확률\" 벡터 추가하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOpsts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
       " ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
       " ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
       " ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
       " ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
       " ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOpsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전에 불러왔던 값을 데이터로 불러옴\n",
    "myVocabList = createVocabList(listOpsts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myVocabList에 있는 단어들을 가자ㅣ고 하나의 리스트를 생성하게 된다.\n",
    "trainMat = []\n",
    "for postinDoc in listOpsts: \n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " 여기서 for 반복문은 tarinMat 리스트를 단어 벡터로 채운다. 이제 폭력적인 단어가 있는 문서의 확률과 두 개의 확률 벡터를 구해 보도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(c_1)$ : 폭력적인 단어가 있는 문서의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04452244, -2.35137526, -2.35137526, -3.04452244, -2.35137526,\n",
       "       -1.94591015, -3.04452244, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -1.65822808, -2.35137526, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -2.35137526, -1.94591015,\n",
       "       -3.04452244, -3.04452244, -3.04452244, -2.35137526, -2.35137526,\n",
       "       -2.35137526, -2.35137526])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 폭력적인 단어가 있는 문서일때, 각 단어가 가지는 우도를 나타냄\n",
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.56494936, -3.25809654, -2.15948425, -2.56494936, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -3.25809654, -2.56494936, -3.25809654, -3.25809654,\n",
       "       -2.56494936, -2.56494936, -1.87180218, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -3.25809654])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 폭력적인 단어 없는 문서일때, 각 단어가 가지는 우도를 나타냄\n",
    "p0V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이제 분류기 전체를 구축하도록 준비**\n",
    "#### [리스팅 4.3] 나이브 베이즈 분류 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네 개의 입력 변수를 받음\n",
    "# vec2Classify : 분류를 위한 벡터\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat = []\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 준비: 중복 단어 문서 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $ \\ $ 앞서 `setOfWords2Vec()`는 하나의 문서에 2개 이상의 중복된 단어가 있어도 무조건 1으로만 나타냄, 이는 불합리함. 이를 해결하기 위해서 `bagOfWords2VecMN()` 라는 함수를 사용 -> 단어 벡터를 1로 설정한 것보다 단어 벡터가 증가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 예제 : 스팸 이메일 분류하기 \n",
    "\n",
    "#### $ \\ $ 앞선 예제는 list로 정형화된 데이터를 가지고 했지만, 실생활 문제에서 나이브 베이스를 사용하기 위해서는 문서 전체를 스트링 리스트로 변환할 수 있어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 :\n",
    "1. 수집: 제공된 텍스트 파일\n",
    "2. 준비: 토큰 벡터로 텍스트 구문 분석\n",
    "3. 분석: 구문 분석이 정확하게 되었는지 토큰 검토\n",
    "4. 훈련: 이전에 생성했던 trainNB0()사용\n",
    "5. 검사: classifyNB()를 사용하고 문서 집합에서 오류율을 계산하는 새로운 검사 함수를 생성한다.\n",
    "6. 사용: 완전한 프로그램을 구축하여 문서들을 분류하고 화면에 잘못 분류된 문서들을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 준비: 텍스트 토큰 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon'\n",
    "mySent.split()\n",
    "\n",
    "###문제점 : 구두점이 단어의 일부로 간주됨.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위의 문제점을 해결하기 위해 `re`라는 라이브러리를 활용해 효과적인 분할**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " '',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\WW*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 제거해야 할 약간의 빈 문자열을 가지고 있으므로, 우리는 각 스트링의 길이를 구할 수 있고, 길이가 0보다 큰 아이템만을 반환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok for tok in listOfTokens if len(tok) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.2 검사 : 나이브 베이즈로 교차 검증하기\n",
    "\n",
    "#### [리스팅4.5] : 파일 구문 분석과 전체 스팸 검사 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):    #input is big string, #output is word list\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "def spamTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet = []           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(list(trainingSet)[randIndex])\n",
    "    trainMat = []; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))\n",
    "    #return vocabList, fullText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
