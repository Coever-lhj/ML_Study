{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-SVM-(Linear-SVN)\" data-toc-modified-id=\"Introduction-to-SVM-(Linear-SVN)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to SVM (Linear SVN)</a></span><ul class=\"toc-item\"><li><span><a href=\"#hyperplane(초평면)\" data-toc-modified-id=\"hyperplane(초평면)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>hyperplane(초평면)</a></span></li><li><span><a href=\"#Margin(마진)\" data-toc-modified-id=\"Margin(마진)-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Margin(마진)</a></span></li><li><span><a href=\"#목적식과-제약식-정의\" data-toc-modified-id=\"목적식과-제약식-정의-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>목적식과 제약식 정의</a></span></li><li><span><a href=\"#라그랑지안-문제로-변환\" data-toc-modified-id=\"라그랑지안-문제로-변환-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>라그랑지안 문제로 변환</a></span></li><li><span><a href=\"#Dual-문제로-변환\" data-toc-modified-id=\"Dual-문제로-변환-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Dual 문제로 변환</a></span></li><li><span><a href=\"#SVM의-해\" data-toc-modified-id=\"SVM의-해-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>SVM의 해</a></span></li></ul></li><li><span><a href=\"#C-SVM-:-Imperfect-seperation\" data-toc-modified-id=\"C-SVM-:-Imperfect-seperation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>C-SVM : Imperfect seperation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/contents.png\" width=\"600\" height=\"500\"/> \n",
    "\n",
    "## Introduction to SVM (Linear SVN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperplane(초평면)\n",
    "\n",
    "$ \\ $ 두 범주를 나누는 분류 문제를 푼다고 가정해 보겠습니다. 아래 그림에서 직선이 두 클래스를 무난하게 분류하고 있음을 확인할 수 있습니다.\n",
    "\n",
    "<img src=\"image/hyperplabe.jpg\" width=\"300\" height=\"300\"/> \n",
    "\n",
    "$ \\ $ 아래 그림 **B, C, D** 중 어느 **hyperplane**이 더 클래스를 잘 분류할까요? \n",
    "\n",
    "<img src=\"image/m_hyperplabe.jpg\" width=\"400\" height=\"400\"/> \n",
    "\n",
    "$ \\ $ 정답은 **D**로 나머지 **hyperplane**보다 확연하게 분류를 합니다.\n",
    "\n",
    "### Margin(마진)\n",
    "\n",
    "$ \\ $ 위에서 초평면을 언급했지만, Margin을 설명하기 위해 다시 한번 아래 그림을 봐주세요. 그림에서 초평면 B1과 B2 모두 두 클래스를 무난하게 분류하고 있음을 확인할 수 있습니다.\n",
    "\n",
    "<img src=\"image/margin.png\" width=\"400\" height=\"400\"/> \n",
    "\n",
    "$ \\ $ 위 그림에서 **b12**을 **minus-plane**, **b11**을 **plus-plane**, 이 둘 사이의 거리를 **마진(margin)**이라고 합니다. **SVM은 이 마진을 최대화하는 분류 경계면을 찾는 기법입니다. ** 이를 도식적으로 나타내면 아래와 같습니다.\n",
    "\n",
    "<img src=\"image/margin2.png\" width=\"400\" height=\"600\"/> \n",
    "\n",
    "$ \\ $ 그럼 마진의 길이가 얼마인지 유도해보겠습니다. 우선 우리가 찾아야 하는 분류경계면을 **$w^Tx + b$** 라고 둡시다. 그러면 **벡터 $w$는 이 경계면과 수직인 법선벡터**가 됩니다.\n",
    "\n",
    "$ \\ $  **$w$** 를 2차원 벡터 $(w_1,w_2)^T$라고 두겠습니다. **$w$** 에 대해 원점과의 거리가 $b$인 직선의 방정식은 $w^Tx + b$ = $w_1x_1 + w_2x_2 + b = 0$ 이 됩니다. 이 직선의 기울기는 $-\\frac{w_1}{w_2}$이고, 법선벡터 $w$의 기울기는 $\\frac{w_2}{w_1}$ 이므로 두 직선은 서로 수직입니다. 이를 차원을 확장하여 생각해도 마찬가지 입니다.\n",
    "\n",
    "$ \\ $ 어쨌든 이 사실을 바탕으로 plus-plane 위에 있는 벡터 $x^+$ 와 $x^−$ 사이의 관계를 다음과 같이 정의할 수 있습니다. $x^−$를 $w$ 방향으로 **평행이동시키되** 이동 폭은 $\\lambda$로 **스케일**한다는 취지입니다.\n",
    "\n",
    "$$\n",
    "x^+ = x^- + \\lambda w\n",
    "$$\n",
    "\n",
    "$ \\ $ 그럼 $\\lambda$은 어떤 값을 지닐까요? $x^+$는 plus-plane, $x^-$는 minus-plane 위에 있다는 사실과 $x^+$와 $x^−$ 사이의 관계식을 활용하면 다음과 같이 유도해낼 수 있습니다.\n",
    "\n",
    "<img src=\"image/margin3.png\" width=\"300\" height=\"300\"/> \n",
    "\n",
    "$ \\ $ 마진은 plus-plane과 minus-plane 사이의 거리를 의미합니다. 이는 $x^+$와 $x^-$ 사이의 거리와 같습니다. 둘 사이의 관계식과 $\\lambda$값을 알고 있으므로 식을 정리하면 마진을 다음과 같이 유도할 수 있습니다.\n",
    "\n",
    "<img src=\"image/margin4.png\" width=\"300\" height=\"300\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적식과 제약식 정의\n",
    "\n",
    "$ \\ $ SVM의 목적은 마진을 최대화하는 경계면을 찾는 것입니다. 계산상 편의를 위해 마진 절반을 제곱한 것에 역수를 취한 뒤 그 절반을 최소화하는 문제로 바꾸겠습니다. 이렇게 해도 문제의 본질은 바뀌지 않습니다.\n",
    "\n",
    "**object function (목적식)**\n",
    "$$\n",
    "max \\frac{2}{||w||_2} \\rightarrow min \\frac{1}{2}w^Tw\n",
    "$$\n",
    "\n",
    "**subject to (제약식)**\n",
    "$$\n",
    "y_i(w^Tx_i+b)≥1 \n",
    "$$\n",
    "\n",
    "$ \\ $ 여기엔 다음과 같은 제약조건이 관측치 개수만큼 붙습니다. 식의 의미는 이렇습니다. plus-plane보다 위에 있는 관측치들은 y=1이고 $w^Tx+b$가 1보다 큽니다. 반대로 minus-plane보다 아래에 있는 점들은 y=−1이고 $w^Tx+b$가 -1보다 작습니다. 이 두 조건을 한꺼번에 묶으면 위와 같은 제약식이 됩니다.\n",
    "\n",
    "___\n",
    "\n",
    "$ \\ $ **This is a convex, [quadratic programming problem]()(in $w$, $b$), in a [convex set](https://ratsgo.github.io/convex%20optimization/2017/12/25/convexset/).**  \n",
    "**Introducing Lagrange multipliers $\\alpha_1$, $\\alpha_2$, $\\alpha_3$, ... $\\alpha_N$ $\\ge$ 0, we have following Lagrangian:**\n",
    "\n",
    "\n",
    "$$\n",
    "min \\ L_p(w,b,\\alpha_i) = \\frac{1}{2}w^Tw - \\sum_{i=1}^N \\alpha_i y_i(wx_i+b) + \\sum_{i=1}^N \\alpha_i \n",
    "$$\n",
    "\n",
    "___\n",
    "\n",
    "> 라그랑지안이 갑자기 나와서 많이들 놀라셨을텐데 겁내지 마시고 아래 동영상을 보면서 차분히 접근해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/yuqB-d5MjZA\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "# 설정 - 자막 - 자동번역 - 원하는 언어 선택\n",
    "HTML('<iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/yuqB-d5MjZA\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라그랑지안 문제로 변환\n",
    "\n",
    "\n",
    "$ \\ $ 라그랑지안 승수법(Lagrange multiplier method)은 제약식에 형식적인 라그랑지안 승수를 곱한 항을 최적화하려는 목적식에 더하여, **제약된 문제를 제약이 없는 문제로 바꾸는 기법**입니다. 이에 대해 추가적인 내용은 [이곳](https://datascienceschool.net/view-notebook/0c66f1810445488baf19cac79305793b/)을 참고하면 좋을 것 같습니다. \n",
    "\n",
    "> **데이터 사이언스 스쿨**들어가셔서 두번째 내용인 **[부등식 제한 조건이 있는 최적화 문제]** 중심으로 보세요.\n",
    "\n",
    "**object function (목적식)**\n",
    "\n",
    "$$\n",
    "min \\ L_P(w,b,\\alpha_i) = \\frac{1}{2}w^Tw - \\sum_{i=1}^N \\alpha_i y_i(wx_i+b) + \\sum_{i=1}^N \\alpha_i \n",
    "$$\n",
    "\n",
    "**subject to (제약식)**\n",
    "\n",
    "$$\n",
    "\\alpha_i \\ge 0, \\qquad i = 1, .... N\n",
    "$$\n",
    "\n",
    "위의 제약식은 KKT(Karush-Kuhn-Tucker) 조건 중 **(3) 음수가 아닌 라그랑지 승수** 조건에 의해 생성 -> ???????(확인 필요)\n",
    "\n",
    "### Dual 문제로 변환\n",
    "\n",
    "$ \\ $ KKT 조건에서는 $L_p(w,b,\\alpha_i)$ 를 미지수로 각각 편미분한 식이 0이 되는 지점에서 최소값을 갖습니다. 다음과 같습니다.\n",
    "\n",
    "<img src=\"image/dual1.png\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "$ \\ $ 위 식을 $L_p(w,b,\\alpha_i)$ 에 넣어 정리해 보겠습니다.  \n",
    "\n",
    "\n",
    "우선 **첫번째 항**부터 보겠습니다.\n",
    "\n",
    "<img src=\"image/dual2.png\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "$ \\ $ 이번엔 **두번째 항**입니다.\n",
    "\n",
    "<img src=\"image/dual3.png\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "$ \\ $ 지금까지 도출한 결과를 토대로 $L_p(w,b,\\alpha_i)$를 정리하면 다음과 같습니다. 식을 변형하는 과정에서 $\\alpha$에 관한 식으로 간단해졌습니다. $\\alpha$의 최고차항의 계수가 음수이므로 최소값을 찾는 문제가 최대값을 찾는 문제로 바뀌었습니다. 이로써 **Dual 문제**로 변환된 것입니다.\n",
    "\n",
    "$$\n",
    "max \\ L_D(\\alpha_i) = \\sum_{i=1}^N \\alpha_i \\ - \\ \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jx_i^Tx_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\ $ KKT 조건에 의해 $L_D$ 의 제약식은 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\alpha_iy_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_i \\ge 0, \\qquad i = 1, .... N\n",
    "$$\n",
    "\n",
    "> 혹시 위에서 언급한 primal, daul에 대한 $L_p(w,b,\\alpha_i)$ 가 $\\ L_D(\\alpha_i)$로 변하는 과정에 대해 감이 안오신다면 [[쌍대이론과 민감도]](http://secom.hanbat.ac.kr/or/ch04/right04.html)을 참고하세요.\n",
    ">\n",
    "> \"선형계획법\" 예제입니다. 출퇴근하시면서 보시면 좋을 것 같습니다. 혹시나 궁금하신점 있으시면 일요일 전에 미리 말씀해주세요(지난학기에 재수강??..)기억이 나려고 해요....\n",
    "\n",
    "### SVM의 해\n",
    "\n",
    "$ \\ $ 우리가 찾고자 한 답은 마진이 최대화된 분류경계면 $w^Tx+b$입니다. $w$와 $b$를 찾으면 SVM의 해를 구할 수 있게 됩니다. KKT 조건을 탐색하는 과정에서 $w$는 다음과 같이 도출됐습니다.\n",
    "\n",
    "$$\n",
    "W =  \\sum_{i=1}^N \\alpha_iy_ix_i \n",
    "$$\n",
    "\n",
    "$ \\ $ $x_i$와 $y_i$는 우리가 가지고 있는 학습데이터이므로 라그랑지안 승수인 $\\alpha$ 값들만 알면 $w$를 찾을 수 있습니다. 그런데 여기에서 $\\alpha_i$가 0인 관측치들은 분류경계면 형성에 아무런 영향을 끼치지 못합니다. 바꿔 말해 $i$번째 관측치에 대응하는 라그랑지안 승수$\\alpha_i$가 0보다 커야 마진 결정에 유의미하다는 이야기입니다.\n",
    "\n",
    "$ \\ $ 아울러 KKT 조건에 의해 해당 함수가 최적값을 갖는다면 아래 두 개 가운데 하나는 반드시 0입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha_i = 0 \\ 또는 \\  y_i(w^Tx_i + b -1) = 0\n",
    "$$\n",
    "\n",
    "$ \\ $ $\\alpha_i$ 가 0이 아니라면 $y_i(w^Tx_i + b -1)$가 반드시 0입니다. 따라서 $x_i$는 plus-plane 또는 minus-plane 위에 있는 벡터가 됩니다. 이렇게 마진 결정에 영향을 끼치는 관측치들을 서포트 벡터(support vectors)라고 합니다. 아래 그림과 같습니다.\n",
    "\n",
    "<img src=\"image/svm.png\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "$ \\ $ 한편 $b$는 이미 구한 $w$와 학습데이터, $y_i(w^Tx_i + b -1)=0$ 식을 활용해 바로 구할 수 있게 됩니다. 새로운 데이터가 들어왔을 때는 해당 관측치를 $y_i(w^Tx_i + b -1)$ 에 넣어서 0보다 크면 1, 0보다 작으면 -1 범주로 예측하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-SVM : Imperfect seperation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
